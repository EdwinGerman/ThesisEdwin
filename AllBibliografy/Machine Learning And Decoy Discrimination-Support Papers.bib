Automatically generated by Mendeley Desktop 1.17.10
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Nekkaa2015,
abstract = {The memetic algoruthm (MA) is an evolutionary metaheuristic that can be viewed as a hybrid genetic algorithm combined with some kinds of local search. In this paper, we propose a memetic algorithm combined with a support vector machine(SVM) for feature selection and classification in Data mining. The proposed approach tries to find a subset of features that maximizes the classification accuracy rate of SVM. In addition, another hybrid algorithm of MA and SVM with optmized parameters is also developed. The two versions of our proposed method are evaluated on some datasets and compared with some well-known classifiers for data classification. The computational experiments show that the hybrid method MA + SVM with optimized parameters provides competitive results and finds high quality solutions.},
author = {Nekkaa, Messaouda and Boughaci, Dalila},
doi = {10.1007/s12293-015-0153-2},
file = {:home/edwin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nekkaa, Boughaci - 2015 - A memetic algorithm with support vector machine for feature selection and classification.pdf:pdf},
issn = {1865-9284},
journal = {Memetic Computing},
keywords = {classification,cross,data mining,feature selection,ga,genetic algorithm,ma,machine,memetic algorithm,parameters optimization,search,sls,stochastic local,support vector,svm,validation},
number = {1},
pages = {59--73},
title = {{A memetic algorithm with support vector machine for feature selection and classification}},
url = {http://link.springer.com/10.1007/s12293-015-0153-2},
volume = {7},
year = {2015}
}
@techreport{Harik1999,
abstract = {From the user's point of view, setting the parameters of a genetic algorithm (GA) is far from a trivial task. Moreover, the user is typically not interested in population sizes, crossover probabilities, selection rates, and other GA technicalities. He is just interested in solving a problem, and what he would really like to do, is to hand-in the problem to a blackbox algorithm, and simply press a start button. This paper explores the development of a GA that fulfills this requirement. It has no ...},
author = {Harik, Georges R and Lobo, Fernando G.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
doi = {10.1016/j.ins.2003.03.029},
file = {:home/edwin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Harik, Lobo - 1999 - A Parameter-less genetic algorithm.pdf:pdf},
isbn = {1-55860-611-4},
keywords = {ga,measure,parameters{\_}analysis},
pages = {258--265},
title = {{A Parameter-less genetic algorithm}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.50.3269},
volume = {1},
year = {1999}
}
@article{Vafaie1994,
author = {Vafaie, Haleh and Imam, If F},
file = {:home/edwin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vafaie, Imam - 1994 - Feature Selection Methods Genetic Algorithms vs Greedy-like Search.pdf:pdf},
journal = {Proceedings of the International Conference on Fuzzy and Intelligent Control Systems},
keywords = {feature selection,genetic algorithms,machine learning,search},
number = {March},
title = {{Feature Selection Methods: Genetic Algorithms vs Greedy-like Search}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.160.9710{\&}rep=rep1{\&}type=pdf},
volume = {1},
year = {1994}
}
@article{Mukhopadhyay2014,
abstract = {This paper is the second part of a two-part paper, which is a survey of multiobjective evolutionary algorithms for data mining problems. In Part I , multiobjective evolutionary algorithms used for feature selection and classification have been reviewed. In this part, different multiobjective evolutionary algorithms used for clustering, association rule mining, and other data mining tasks are surveyed. Moreover, a general discussion is provided along with scopes for future research in the domain of multiobjective evolutionary algorithms for data mining.},
author = {Mukhopadhyay, Anirban and Maulik, Ujjwal and Bandyopadhyay, Sanghamitra and Coello, Carlos a Coello},
doi = {10.1109/TEVC.2013.2290082},
file = {:home/edwin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mukhopadhyay et al. - 2014 - Survey of multiobjective evolutionary algorithms for data mining Part II.pdf:pdf},
issn = {1089778X},
journal = {IEEE Transactions on Evolutionary Computation},
keywords = {Association rule mining,biclustering,clustering,ensemble learning,multiobjective evolutionary algorithms},
number = {1},
pages = {25--35},
title = {{Survey of multiobjective evolutionary algorithms for data mining: Part II}},
volume = {18},
year = {2014}
}
@article{Lanzi1997,
author = {Lanzi, Pier Luca and Milano, Politecnico},
file = {:home/edwin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lanzi, Milano - 1997 - Fast Feature Selection with Genetic Algorithms A Filter Approach.pdf:pdf},
isbn = {0780339495},
journal = {IEEE Transaction on Computers},
pages = {537--540},
title = {{Fast Feature Selection with Genetic Algorithms : A Filter Approach}},
year = {1997}
}
@article{Saeys2007,
abstract = {Feature selection techniques have become an apparent need in many bioinformatics applications. In addition to the large pool of techniques that have already been developed in the machine learning and data mining fields, specific applications in bioinformatics have led to a wealth of newly proposed techniques. In this article, we make the interested reader aware of the possibilities of feature selection, providing a basic taxonomy of feature selection techniques, and discussing their use, variety and potential in a number of both common as well as upcoming bioinformatics applications.},
author = {Saeys, Yvan and Inza, I{\~{n}}aki and Larra{\~{n}}aga, Pedro},
doi = {10.1093/bioinformatics/btm344},
file = {:home/edwin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Saeys, Inza, Larra{\~{n}}aga - 2007 - A review of feature selection techniques in bioinformatics.pdf:pdf},
isbn = {1367-4811 (Electronic)$\backslash$r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {19},
pages = {2507--2517},
pmid = {17720704},
title = {{A review of feature selection techniques in bioinformatics}},
volume = {23},
year = {2007}
}
@article{Qin2009,
abstract = {Differential evolution (DE) is an efficient and powerful population-based stochastic search technique for solving optimization problems over continuous space, which has been widely applied in many scientific and engineering fields. However, the success of DE in solving a specific problem crucially depends on appropriately choosing trial vector generation strategies and their associated control parameter values. Employing a trial-and-error scheme to search for the most suitable strategy and its associated parameter settings requires high computational costs. Moreover, at different stages of evolution, different strategies coupled with different parameter settings may be required in order to achieve the best performance. In this paper, we propose a self-adaptive DE (SaDE) algorithm, in which both trial vector generation strategies and their associated control parameter values are gradually self-adapted by learning from their previous experiences in generating promising solutions. Consequently, a more suitable generation strategy along with its parameter settings can be determined adaptively to match different phases of the search process/evolution. The performance of the SaDE algorithm is extensively evaluated (using codes available from P. N. Suganthan) on a suite of 26 bound-constrained numerical optimization problems and compares favorably with the conventional DE and several state-of-the-art parameter adaptive DE variants.},
author = {Qin, a. K. and Huang, V. L. and Suganthan, P. N.},
doi = {10.1109/TEVC.2008.927706},
file = {:home/edwin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Qin, Huang, Suganthan - 2009 - Differential evolution algorithm with strategy adaptation for global numerical optimization.pdf:pdf},
isbn = {1089-778X VO - 13},
issn = {1089778X},
journal = {IEEE Transactions on Evolutionary Computation},
keywords = {Differential evolution (DE),Global numerical optimization,Parameter adaptation,Self-adaptation,Strategy adaptation},
number = {2},
pages = {398--417},
title = {{Differential evolution algorithm with strategy adaptation for global numerical optimization}},
volume = {13},
year = {2009}
}
@article{Siedlecki1989,
abstract = {We introduce the use of genetic algorithms (GA) for the selection of features in the design of automatic pattern classifiers. Our preliminary results suggest that GA is a powerful means of reducing the time for finding near-optimal subsets of features from large sets.},
author = {Siedlecki, W. and Sklansky, J.},
doi = {10.1016/0167-8655(89)90037-8},
file = {:home/edwin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Siedlecki, Sklansky - 1989 - A note on genetic algorithms for large-scale feature selection.pdf:pdf},
isbn = {0167-8655},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {classifier,feature selection,genetic algorithms,multidimensional data,search},
number = {5},
pages = {335--347},
title = {{A note on genetic algorithms for large-scale feature selection}},
volume = {10},
year = {1989}
}
@inproceedings{A.Mayer2000,
address = {Tshechische Republik - Austria},
author = {{A. Mayer}, Petr Somol},
booktitle = {The 4th Multiconference on Systemics, Cybernetics and Informatics (SCI 2000)},
file = {:home/edwin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/A. Mayer - 2000 - Conventional and Evolutionary Feature Selection of SAR Data Using a Filter Approach.pdf:pdf},
pages = {8},
title = {{Conventional and Evolutionary Feature Selection of SAR Data Using a Filter Approach}},
year = {2000}
}
@article{Hruschka2009,
abstract = {This paper presents a survey of evolutionary algorithms designed for clustering tasks. It tries to reflect the profile of this area by focusing more on those subjects that have been given more importance in the literature. In this context, most of the paper is devoted to partitional algorithms that look for hard clusterings of data, though overlapping (i.e., soft and fuzzy) approaches are also covered in the paper. The paper is original in what concerns two main aspects. First, it provides an up-to-date overview that is fully devoted to evolutionary algorithms for clustering, is not limited to any particular kind of evolutionary approach, and comprises advanced topics like multiobjective and ensemble-based evolutionary clustering. Second, it provides a taxonomy that highlights some very important aspects in the context of evolutionary data clustering, namely, fixed or variable number of clusters, cluster-oriented or nonoriented operators, context-sensitive or context-insensitive operators, guided or unguided operators, binary, integer, or real encodings, centroid-based, medoid-based, label-based, tree-based, or graph-based representations, among others. A number of references are provided that describe applications of evolutionary algorithms for clustering in different domains, such as image processing, computer security, and bioinformatics. The paper ends by addressing some important issues and open questions that can be subject of future research.},
author = {Hruschka, Eduardo Raul and Campello, Ricardo J G B and Freitas, Alex a. and de Carvalho, Andr{\'{e}} C Ponce Leon F},
doi = {10.1109/TSMCC.2008.2007252},
file = {:home/edwin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hruschka et al. - 2009 - A survey of evolutionary algorithms for clustering.pdf:pdf},
isbn = {1094-6977 VO - 39},
issn = {10946977},
journal = {IEEE Transactions on Systems, Man and Cybernetics Part C: Applications and Reviews},
keywords = {Applications,Clustering,Evolutionary algorithms},
number = {2},
pages = {133--155},
title = {{A survey of evolutionary algorithms for clustering}},
volume = {39},
year = {2009}
}
@article{Pellerin2004,
author = {Pellerin, Eric},
doi = {10.1117/12.542156},
file = {:home/edwin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pellerin - 2004 - Self-adaptive parameters in genetic algorithms.pdf:pdf},
issn = {0277786X},
journal = {Proceedings of SPIE},
keywords = {adaptation,evolution,genetic algorithms,learning system,parameter settings,self adaptive parameters},
pages = {53--64},
title = {{Self-adaptive parameters in genetic algorithms}},
url = {http://link.aip.org/link/?PSI/5433/53/1{\&}Agg=doi},
year = {2004}
}
@article{Sastry2005,
abstract = {Genetic algorithms (GAs) are search methods based on principles of natural selection and genetics (Fraser, 1957; Bremermann, 1958; Holland, 1975). We start with a brief introduction to simple genetic algorithms and associated terminology.},
author = {Sastry, Kumara and Goldberg, David and Kendall, Graham},
doi = {10.1007/978-1-60761-842-3_19},
file = {:home/edwin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sastry, Goldberg, Kendall - 2005 - Search Methodologies.pdf:pdf},
isbn = {978-1-4614-6939-1},
issn = {1940-6029},
journal = {Compute},
pages = {97--125},
pmid = {20835807},
title = {{Search Methodologies}},
year = {2005}
}
@article{Liu2005,
abstract = {This paper introduces concepts and algorithms of feature selection, surveys existing feature selection algorithms for classification and clustering, groups and compares different algorithms with a categorizing framework based on search strategies, evaluation criteria, and data mining tasks, reveals unattempted combinations, and provides guidelines in selecting feature selection algorithms. With the categorizing framework, we continue our efforts toward building an integrated system for intelligent feature selection. A unifying platform is proposed as an intermediate step. An illustrative example is presented to show how existing feature selection algorithms can be integrated into a meta algorithm that can take advantage of individual algorithms. An added advantage of doing so is to help a user employ a suitable algorithm without knowing details of each algorithm. Some real-world applications are included to demonstrate the use of feature selection in data mining. We conclude this work by identifying trends and challenges of feature selection research and development},
author = {Liu, Huan and Member, Senior and Yu, Lei and Member, Student},
doi = {10.1109/TKDE.2005.66},
file = {:home/edwin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2005 - Toward integrating feature selection algorithms for classification and clustering.pdf:pdf},
isbn = {1041-4347},
issn = {1041-4347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
number = {4},
pages = {491--502},
title = {{Toward integrating feature selection algorithms for classification and clustering}},
volume = {17},
year = {2005}
}
@article{Deb1999a,
abstract = {Genetic algorithms (GAs) are multi-dimensional and stochastic search methods, involving complex interactions among their parameters. For last two decades, researchers have been trying to understand the mechanics of GA parameter interactions by using various techniques. The methods include careful 'functional' decomposition of parameter interactions, empirical studies, and Markov chain analysis. Although the complex knot of these interactions are getting loose with such analyses, it still remains an open question in the mind of a new-comer to the field or to a GA-practitioner as to what values of GA parameters (such as population size, choice of GA operators, operator probabilities, and others) to use in an arbitrary problem. In this paper, we investigate the performance of simple tripartite GAs on a number of simple to complex test problems from a practical standpoint. Since function evaluations are most time-consuming in a real-world problem, we compare different GAs for a fixed number of function evaluations. Based on probability calculations and simulation results, it is observed that for solving simple problems (unimodal or small modality problems) mutation operator plays an important role, although crossover operator can also solve these problems. However, two operators (when applied alone) have two different working zones for population size. For complex problems involving massive multimodality and misleadingness (deception), crossover operator is the key search operator and performs reliably with an adequate population size. Based on these studies, it is recommended that when in doubt, the use of the crossover operator with an adequate population size is a reliable approach.},
author = {Deb, Kalyanmoy and Agrawal, Samir},
doi = {citeulike-article-id:4372866},
file = {:home/edwin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deb, Agrawal - 1999 - Understanding interactions among genetic algorithm parameters(2).pdf:pdf},
journal = {Foundations of Genetic Algorithms V, San Mateo, CA: Morgan Kauffman},
keywords = {crossover-based,deceptive functions,gas,multi-modal functions,mutation-based gas,population sizing},
pages = {265--286},
title = {{Understanding interactions among genetic algorithm parameters}},
url = {http://repository.ias.ac.in/82722/},
year = {1999}
}
@article{Lobo2004,
abstract = {The parameter-less genetic algorithm was introduced a couple of years ago as a way to simplify genetic algorithm operation by incorporating knowledge of parameter selection and population sizing theory in the genetic algorithm itself. This paper shows how that technique can be used in practice by applying it to a network expansion problem. The existence of the parameter-less genetic algorithm stresses the fact that some problems need more processing power than others. Such observation leads to the development of a problem difficulty measure which is also introduced in this paper. The measure can be useful for comparing the difficulty of real-world problems. ?? 2004 Elsevier Inc. All rights reserved.},
author = {Lobo, Fernando G. and Goldberg, David E.},
doi = {10.1016/j.ins.2003.03.029},
file = {:home/edwin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lobo, Goldberg - 2004 - The parameter-less genetic algorithm in practice.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Genetic algorithms,Network optimization problems,Parameter selection,Problem difficulty},
number = {1-4},
pages = {217--232},
title = {{The parameter-less genetic algorithm in practice}},
volume = {167},
year = {2004}
}
@article{Kotsiantis2011,
author = {Kotsiantis, S. B.},
doi = {10.1007/s10462-011-9230-1},
file = {:home/edwin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kotsiantis - 2011 - RETRACTED ARTICLE Feature selection for machine learning classification problems a recent overview.pdf:pdf},
isbn = {0269-2821},
issn = {0269-2821},
journal = {Artificial Intelligence Review},
keywords = {data mining,machine learning,pattern recognition},
title = {{RETRACTED ARTICLE: Feature selection for machine learning classification problems: a recent overview}},
year = {2011}
}
@article{Xia2014,
abstract = {The multi-objetive evolutionary algorthm (MOEA) has shown remarkable capability of selecting feature subset. Most MOEAs use the cardinality of the feature subset as one of its objetives and adopt a strict Pareto dominance relationship to select individulas. However, these techniques limit available solutions and may omit several appropriate but dominated solutions. A multi-objective unsupervised feature selection algorithm (MOUFSA) is proposed to solve these issues. A new objective, which incorporates the correlation coefficiente and cardinality of the feature subset, not only evaluates the redundancy of selected features but also provides several objetive values for each particular size of feature subset. A relaxed archiving strategy based on negative epsilon-dominated. Three new mutation operators of different abilities are also presented to enhance the algorithm. Nine UCI datasets and five fault recongnition datasets are employed as test objects, and the obtained feature subsets are the used for susequent classification and clustering. Experimental results show that MAUFSA outperforms several other mulyi-objective and tradicional single-objetive methods.},
author = {Xia, Hu and Zhuang, Jian and Yu, Dehong},
doi = {10.1016/j.neucom.2014.06.075},
file = {:home/edwin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xia, Zhuang, Yu - 2014 - Multi-objective unsupervised feature selection algorithm utilizing redundancy measure and negative epsilon-domi.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Fault recognition,Feature selection,Multi-objective evolutionary algorithm,Negative epsilon-dominance,Redundancy measure,multi-objective evolutionary algorithm},
pages = {113--124},
publisher = {Elsevier},
title = {{Multi-objective unsupervised feature selection algorithm utilizing redundancy measure and negative epsilon-dominance for fault diagnosis}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S092523121400873X},
volume = {146},
year = {2014}
}
@article{Mukhopadhyay2014a,
abstract = {The aim of any data mining technique is to build an efficient predictive or descriptive model of a large amount of data. Applications of evolutionary algorithms have been found to be particularly useful for automatic processing of large quantities of raw noisy data for optimal parameter setting and to discover significant and meaningful information. Many real-life data mining problems involve multiple conflicting measures of performance, or objectives, which need to be optimized simultaneously. Under this context, multiobjective evolutionary algorithms are gradually finding more and more applications in the domain of data mining since the beginning of the last decade. In this two-part paper, we have made a comprehensive survey on the recent developments of multiobjective evolutionary algorithms for data mining problems. In this paper, Part I, some basic concepts related to multiobjective optimization and data mining are provided. Subsequently, various multiobjective evolutionary approaches for two major data mining tasks, namely feature selection and classification, are surveyed. In Part II of this paper, we have surveyed different multiobjective evolutionary algorithms for clustering, association rule mining, and several other data mining tasks, and provided a general discussion on the scopes for future research in this domain.},
author = {Mukhopadhyay, Anirban and Maulik, Ujjwal and Bandyopadhyay, Sanghamitra and Coello, Carlos Artemio Coello},
doi = {10.1109/TEVC.2013.2290086},
file = {:home/edwin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mukhopadhyay et al. - 2014 - A survey of multiobjective evolutionary algorithms for data mining Part I.pdf:pdf},
isbn = {1089-778X},
issn = {1089778X},
journal = {IEEE Transactions on Evolutionary Computation},
keywords = {Classification,Pareto optimality,feature selection,multiobjective evolutionary algorithms},
number = {1},
pages = {4--19},
title = {{A survey of multiobjective evolutionary algorithms for data mining: Part I}},
volume = {18},
year = {2014}
}
@article{Oh2004,
abstract = {This paper proposes a novel hybrid genetic algorithm for feature selection. Local search operations are devised and embedded in hybrid GAs to fine-tune the search. The operations are parameterized in terms of their fine-tuning power, and their effectiveness and timing requirements are analyzed and compared. The hybridization technique produces two desirable effects: a significant improvement in the final performance and the acquisition of subset-size control. The hybrid GAs showed better convergence properties compared to the classical GAs. A method of performing rigorous timing analysis was developed, in order to compare the timing requirement of the conventional and the proposed algorithms. Experiments performed with various standard data sets revealed that the proposed hybrid GA is superior to both a simple GA and sequential search algorithms.},
author = {Oh, Il-Seok and Lee, Jin-Seon and Moon, Byung-Ro},
doi = {10.1109/TPAMI.2004.105},
file = {:home/edwin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Oh, Lee, Moon - 2004 - Hybrid genetic algorithms for feature selection.pdf:pdf},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {11},
pages = {1424--1437},
pmid = {15521491},
title = {{Hybrid genetic algorithms for feature selection.}},
volume = {26},
year = {2004}
}
